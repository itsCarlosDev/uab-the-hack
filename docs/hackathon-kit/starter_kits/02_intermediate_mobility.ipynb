{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803e3b4c-4b64-430d-9741-3cde54a2f4ce",
   "metadata": {},
   "source": [
    "### Objectiu i plantejament de la segona part: Descobrir patrons i problemes de connectivitat\n",
    "\n",
    "La nostra proposta és representar gràficament la qualitat de la connexió per APs mitjançant la creació de mapes de calor. I posteriorment, veure quin factor, com ara el nombre de dispositius connectats, la banda de freqüència en ús, etc., influeix en la degradació de la qualitat.\n",
    "\n",
    "Aquesta propietat es mesura principalment a partir de la intensitat de camp/potència de senyal (dBm) trobades a les dades proveïdes de Clients. Es considera confiable a partir d’un valor de -60dBm cap amunt, mentre que un valor per sota d’aquest és considerat més dèbil i pobre a mesura que va decreixent.\n",
    "En l’informe anterior hem visualitzat la majoria de banda de 5GHz sobre la de 2.4GHz pel fet que s’evita saturació i interferències.\n",
    "\n",
    "Com a alternativa, també es pot estudiar la qualitat de connexió a partir del Health Score present en les dades proveïdes de Clients. El Health Score és una mètrica que adopta un valor del 0 al 100 calculat per la pèrdua de bits basant-se en el rendiment, la connectivitat, la cobertura i el nivell d’interferència d’un AP específic, essent així 100 el màxim valor, el qual representa una bona connexió i 0, una dolenta.\n",
    "Intentarem veure la correlació entre les dues característiques, mitjançant altres heatmaps.\n",
    "\n",
    "Per tal de fer això, hem processat les dades de Client.json en ordre de filtrar l’hora, el dia i el dia de la setmana a la qual es connecta l’usuari, el Health Score, la potència de senyal rebuda, i l’AP al que està connectat.\n",
    "Per l’altra banda, processem les dades de AP.json en ordre de filtrar la direcció IP de l’AP, el nombre d’usuaris connectats, la ubicació en format de Latitud/Longitud i l’última que està activa.\n",
    "\n",
    "Així doncs, a més de permetre'ns ubicar cada AP individualment (assenyalat per uns quadrats translúcids de color vermell), obtenim una gràfica que inclou la qualitat de connexió de les més de mil APs que hi ha distribuïdes pel campus en àrees de color (verd per a millor connexió i roig per a pitjor) en intervals d’una hora – de manera que el jurat pot controlar la visualització mitjançant un slide arrossegable del temps. Les àrees pintades les quals estan condicionades per la mitjana de la potència de senyal segons els usuaris connectats.\n",
    "\n",
    "$\\ P_m =($ $\\Sigma P_i)/n$\n",
    "\n",
    "on $\\ P_m$ és la potència de senyal mitjana, $\\ P_i$ són les potències de senyals puntuals i $\\ n$ és el nombre de clients\n",
    "\n",
    "$\\ HS_m =($ $\\Sigma HS_i)/n$ *\n",
    "\n",
    "on $\\ HS_m$ és el Health Score mitjà, $\\ HS_i$ són els Health Scores puntuals i $\\ n$ és el nombre de clients\n",
    "\n",
    "\n",
    "__Observacions:__\n",
    "\n",
    "Això implica que cada AP presentarà una circumferència del mateix diàmetre d’un únic color. D’aquesta manera contemplem el senyal d’una AP com a homogènia i ignorem les possibles interferències com podria ser el tipus i processador dels dispositius connectats o l’obstaculització per parets i objectes sòlids, entre d’altres.\n",
    "\n",
    "\n",
    "L’estratègia que hem seguit ha consistit a veure i comprovar que primer és funcional per a 5 arxius i després intentar escalar el codi i els programes a macroescala.\n",
    "Per dur a terme tot aquest procés, ens hem fet més familiars en l’ús d’LLMs com a eina de suport, hem reforçat els coneixements en plataformes ja conegudes com ara *Python* i a més a més, hem experimentat amb nou material com ara *Folium*.\n",
    "\n",
    "# __Resultats:__\n",
    "\n",
    "Amb les dades introduïdes al seu respectiu codi, obtenim com a resultat tres heatmaps diferents, que aporten tres perspectives diferents pel que fa a la valoració de la qualitat de connexió com a concepte general.\n",
    "\n",
    "El primer de tots tracta la intensitat de camp que reben els clients connectats a una AP específica. Com ja hem esmentat abans, cada dispositiu de Wi-Fi presentarà un únic cercle d’un sol color, el qual té com a llindar el valor de -60dBm (és a dir, si els clients connectats a l’AP reben com a mitjana una potència de -60dBm, el punt serà groc. A mesura que el valor decreix, el color es va ataronjant, arribant a colors vermells si es desvia massa per sota. Anàlogament, el punt adoptarà colors més verds si el senyal que reben els clients es troba per dalt d’aquest límit). Comparant disjuntament, el diàmetre d’aquestes circumferències es mantenen igual per a cada AP.\n",
    "\n",
    "El segon heatmap realitzat es basa en el Health Score dels clients connectats. Això ens permet diferenciar l’obtenció de senyal d’un usuari amb autèntica connexió i qualitat d’aquesta. Dotada de la mateixa dinàmica que abans, el Health Score mitjà d’una AP és representat també amb circumferències de colors de la mateixa mida per a cada, verd per a una major puntuació – 100 i vermell per a valors baixos — 0.\n",
    "\n",
    "L’últim mapa il·lustra el nombre de clients connectats per AP, el qual ens ajuda a detectar les zones més concorregudes i possibles falles de senyal a causa de la massificació de connexió. Hem volgut visualitzar l’augment d’usuaris connectats amb l’expansió del diàmetre de la circumferència assignada per dispositiu, diàmetre més gran equival a més gent connectada i simultàniament, diàmetre més petit, menys gent connectada. Així doncs, aquest últim no presenta diferència de colors, però, en canvi, en modifiquem la grandària dels punts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cf704",
   "metadata": {},
   "source": [
    "1. Filtramos por los parámetros que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility script to build a lightweight JSON snapshot with the\n",
    "fields needed for the ROOKIE analysis walkthrough.\n",
    "\n",
    "It collects data from the AP and Client datasets, keeps only the\n",
    "required columns, and stores them in a single JSON file:\n",
    "\n",
    "{\n",
    "    \"aps\": [{\"timestamp\": \"...\", \"client_count\": ...}, ...],\n",
    "    \"clients\": [\n",
    "        {\"timestamp\": \"...\", \"hour\": 12, \"day_of_week\": \"Monday\", \"date\": \"2025-04-03\"},\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator, List, Optional, Tuple, Dict, Any\n",
    "\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Filter APS and Client datasets into a lightweight JSON dump.\"\n",
    "    )\n",
    "    repo_root = Path(__file__).resolve().parents[1]\n",
    "    parser.add_argument(\n",
    "        \"--aps-dir\",\n",
    "        type=Path,\n",
    "        default=repo_root / \"anonymized_data\" / \"aps\",\n",
    "        help=\"Directory containing AP snapshot JSON files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--clients-dir\",\n",
    "        type=Path,\n",
    "        default=repo_root / \"anonymized_data\" / \"clients\",\n",
    "        help=\"Directory containing client snapshot JSON files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=Path,\n",
    "        default=repo_root / \"data\" / \"rookie_filtered_dataset.json\",\n",
    "        help=\"Output JSON file path.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-aps-files\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Optional limit for AP files (useful for quick tests).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-client-files\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Optional limit for client files.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--aps-output\",\n",
    "        type=Path,\n",
    "        default=repo_root / \"data\" / \"rookie_filtered_aps.json\",\n",
    "        help=\"Path for the AP-only JSON output.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--clients-output\",\n",
    "        type=Path,\n",
    "        default=repo_root / \"data\" / \"rookie_filtered_clients.json\",\n",
    "        help=\"Path for the client-only JSON output.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip-combined\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Skip writing the combined JSON payload.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--aps-geojson\",\n",
    "        type=Path,\n",
    "        default=repo_root.parent / \"geolocation_package\" / \"data\" / \"aps_geolocalizados_etrs89.geojson\",\n",
    "        help=\"GeoJSON file providing AP location metadata.\",\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def iter_json_files(directory: Path, max_files: Optional[int] = None) -> Iterator[Path]:\n",
    "    files: List[Path] = sorted(directory.glob(\"*.json\"))\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "    for file in files:\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "        yield file\n",
    "\n",
    "\n",
    "def iter_json_records(files: Iterable[Path]) -> Iterator[dict]:\n",
    "    for file in files:\n",
    "        with file.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "            try:\n",
    "                data = json.load(handle)\n",
    "            except json.JSONDecodeError as exc:\n",
    "                raise ValueError(f\"Invalid JSON in {file}: {exc}\") from exc\n",
    "        if isinstance(data, list):\n",
    "            for record in data:\n",
    "                if isinstance(record, dict):\n",
    "                    yield record\n",
    "\n",
    "\n",
    "def load_geo_index(geojson_path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    if not geojson_path.exists():\n",
    "        return {}\n",
    "    with geojson_path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        payload = json.load(handle)\n",
    "    features = payload.get(\"features\", [])\n",
    "    index: Dict[str, Dict[str, Any]] = {}\n",
    "    for feature in features:\n",
    "        props = feature.get(\"properties\", {})\n",
    "        ap_name = props.get(\"USER_NOM_A\")\n",
    "        if not ap_name:\n",
    "            continue\n",
    "        index[ap_name] = {\n",
    "            \"space\": props.get(\"USER_Espai\"),\n",
    "            \"building_code\": props.get(\"Nom_Edific\"),\n",
    "            \"building_name\": props.get(\"USER_EDIFI\"),\n",
    "            \"floor\": props.get(\"Num_Planta\"),\n",
    "            \"short_ref\": props.get(\"Ref_Curta\"),\n",
    "            \"x\": props.get(\"X\"),\n",
    "            \"y\": props.get(\"Y\"),\n",
    "        }\n",
    "    return index\n",
    "\n",
    "\n",
    "def build_aps_slice(\n",
    "    directory: Path, max_files: Optional[int], geo_index: Dict[str, Dict[str, Any]]\n",
    ") -> Tuple[List[dict], int]:\n",
    "    files = list(iter_json_files(directory, max_files))\n",
    "    results: List[dict] = []\n",
    "    for record in iter_json_records(files):\n",
    "        last_modified = record.get(\"last_modified\")\n",
    "        client_count = record.get(\"client_count\")\n",
    "        if last_modified is None:\n",
    "            continue\n",
    "        try:\n",
    "            ts = datetime.fromtimestamp(float(last_modified), tz=timezone.utc)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "        ts_date = ts.date().isoformat()\n",
    "        ts_time = ts.time().isoformat(timespec=\"seconds\")\n",
    "        ap_name = record.get(\"name\")\n",
    "        results.append(\n",
    "            {\n",
    "                \"name\": ap_name,\n",
    "                \"serial\": record.get(\"serial\"),\n",
    "                \"timestamp\": ts.isoformat(),\n",
    "                \"date\": ts_date,\n",
    "                \"time\": ts_time,\n",
    "                \"client_count\": client_count,\n",
    "                \"location\": geo_index.get(ap_name),\n",
    "            }\n",
    "        )\n",
    "    return results, len(files)\n",
    "\n",
    "\n",
    "def build_clients_slice(directory: Path, max_files: Optional[int]) -> Tuple[List[dict], int]:\n",
    "    files = list(iter_json_files(directory, max_files))\n",
    "    results: List[dict] = []\n",
    "    for record in iter_json_records(files):\n",
    "        last_connection = record.get(\"last_connection_time\")\n",
    "        if last_connection is None:\n",
    "            continue\n",
    "        try:\n",
    "            # Convert from milliseconds to seconds.\n",
    "            ts = datetime.fromtimestamp(float(last_connection) / 1000, tz=timezone.utc)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "        rounded_hour = ts.hour + (1 if ts.minute >= 30 else 0)\n",
    "        rounded_hour = rounded_hour % 24\n",
    "        results.append(\n",
    "            {\n",
    "                \"timestamp\": ts.isoformat(),\n",
    "                \"hour\": rounded_hour,\n",
    "                \"day_of_week\": ts.strftime(\"%A\"),\n",
    "                \"date\": ts.date().isoformat(),\n",
    "                \"dia\": ts.day,\n",
    "                \"health\": record.get(\"health\"),\n",
    "                \"signal_db\": record.get(\"signal_db\"),\n",
    "                \"associated_device_name\": record.get(\"associated_device_name\"),\n",
    "            }\n",
    "        )\n",
    "    return results, len(files)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "    geo_index = load_geo_index(args.aps_geojson)\n",
    "    aps_slice, aps_files_count = build_aps_slice(\n",
    "        args.aps_dir, args.max_aps_files, geo_index\n",
    "    )\n",
    "    clients_slice, client_files_count = build_clients_slice(\n",
    "        args.clients_dir, args.max_client_files\n",
    "    )\n",
    "\n",
    "    outputs_written = []\n",
    "\n",
    "    if not args.skip_combined and args.output:\n",
    "        output_path: Path = args.output\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        payload = {\n",
    "            \"aps\": aps_slice,\n",
    "            \"clients\": clients_slice,\n",
    "            \"meta\": {\n",
    "                \"aps_files\": aps_files_count,\n",
    "                \"client_files\": client_files_count,\n",
    "            },\n",
    "        }\n",
    "        with output_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "            json.dump(payload, handle, ensure_ascii=True, indent=2)\n",
    "        outputs_written.append(\n",
    "            f\"Combined JSON â†’ {output_path} (APS {len(aps_slice)}, Clients {len(clients_slice)})\"\n",
    "        )\n",
    "\n",
    "    if args.aps_output:\n",
    "        aps_path: Path = args.aps_output\n",
    "        aps_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with aps_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "            json.dump(aps_slice, handle, ensure_ascii=True, indent=2)\n",
    "        outputs_written.append(f\"AP slice â†’ {aps_path} ({len(aps_slice)} registros)\")\n",
    "\n",
    "    if args.clients_output:\n",
    "        clients_path: Path = args.clients_output\n",
    "        clients_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with clients_path.open(\"w\", encoding=\"utf-8\") as handle:\n",
    "            json.dump(clients_slice, handle, ensure_ascii=True, indent=2)\n",
    "        outputs_written.append(\n",
    "            f\"Client slice â†’ {clients_path} ({len(clients_slice)} registros)\"\n",
    "        )\n",
    "\n",
    "    print(\"âœ… JSON generado:\")\n",
    "    for line in outputs_written:\n",
    "        print(f\"   â€¢ {line}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e4ff5",
   "metadata": {},
   "source": [
    "2. Construimos las gráficas y las dejamos en formato html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e508318d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfolium\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfolium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimestampedGeoJson\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyproj\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "from pyproj import Transformer\n",
    "import json\n",
    "import branca # Necesario para las escalas de color\n",
    "import numpy as np # Necesario para comprobar NaNs\n",
    "\n",
    "# --- Constantes ---\n",
    "FILE_APS = 'rookie_filtered_aps.json'\n",
    "FILE_CLIENTS = 'rookie_filtered_clients.json'\n",
    "\n",
    "# Archivos de salida\n",
    "OUTPUT_MAP_HEALTH = 'mapa_health_dinamico.html'\n",
    "OUTPUT_MAP_SIGNAL = 'mapa_signal_dinamico.html'\n",
    "OUTPUT_MAP_CLIENTS = 'mapa_clientes_dinamico.html'\n",
    "\n",
    "# --- Función de Escala (para el radio) ---\n",
    "def linear_scale(value, in_min, in_max, out_min, out_max):\n",
    "    \"\"\"\n",
    "    Mapea un valor de un rango a otro (interpolación lineal).\n",
    "    Usado para calcular el radio del círculo.\n",
    "    \"\"\"\n",
    "    if in_min == in_max:\n",
    "        return (out_min + out_max) / 2\n",
    "    clamped_value = max(in_min, min(value, in_max))\n",
    "    in_range = in_max - in_min\n",
    "    out_range = out_max - out_min\n",
    "    scaled_value = (clamped_value - in_min) / in_range\n",
    "    return out_min + (scaled_value * out_range)\n",
    "\n",
    "# Definimos el conversor de coordenadas.\n",
    "try:\n",
    "    transformer = Transformer.from_crs(\"epsg:25831\", \"epsg:4326\")\n",
    "except ImportError:\n",
    "    print(\"Error: La librería 'pyproj' no está instalada.\")\n",
    "    print(\"Por favor, instálala ejecutando: py -m pip install pyproj branca\")\n",
    "    exit()\n",
    "\n",
    "# --- ¡MODIFICADO! Coordenadas de inicio ---\n",
    "# Coordenadas de AP-VET71\n",
    "start_x, start_y = 424638.107049, 4595093.80301\n",
    "start_lat, start_lon = transformer.transform(start_x, start_y)\n",
    "map_center_coords = [start_lat, start_lon]\n",
    "# ------------------------------------\n",
    "\n",
    "print(\"Script iniciado...\")\n",
    "\n",
    "# --- 1. Cargar y Procesar Datos de Clientes ---\n",
    "print(f\"Cargando y procesando clientes desde {FILE_CLIENTS}...\")\n",
    "try:\n",
    "    df_clients = pd.read_json(FILE_CLIENTS)\n",
    "    \n",
    "    df_clients['health'] = pd.to_numeric(df_clients['health'], errors='coerce')\n",
    "    df_clients['signal_db'] = pd.to_numeric(df_clients['signal_db'], errors='coerce')\n",
    "    df_clients = df_clients.dropna(subset=['health', 'signal_db', 'associated_device_name', 'date', 'hour'])\n",
    "\n",
    "    df_metrics = df_clients.groupby(['date', 'hour', 'associated_device_name']).agg(\n",
    "        avg_health=('health', 'mean'),\n",
    "        avg_signal_db=('signal_db', 'mean'),\n",
    "        num_clients_metricos=('health', 'size')\n",
    "    ).reset_index()\n",
    "\n",
    "    df_metrics.rename(columns={'associated_device_name': 'name'}, inplace=True)\n",
    "    print(f\"Métricas de clientes calculadas (ej: {len(df_metrics)} registros de AP/hora).\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo {FILE_CLIENTS}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error procesando {FILE_CLIENTS}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Cargar y Procesar Ubicaciones de APs ---\n",
    "print(f\"Cargando y procesando APs desde {FILE_APS}...\")\n",
    "try:\n",
    "    with open(FILE_APS, 'r', encoding='utf-8') as f:\n",
    "        data_aps = json.load(f)\n",
    "    df_aps = pd.DataFrame(data_aps)\n",
    "\n",
    "    df_aps = df_aps.dropna(subset=['location'])\n",
    "    df_ap_locations = df_aps.drop_duplicates(subset=['name'], keep='last').copy()\n",
    "\n",
    "    def convert_coordinates(row):\n",
    "        try:\n",
    "            x = row['location']['x']\n",
    "            y = row['location']['y']\n",
    "            lat, lon = transformer.transform(x, y)\n",
    "            return pd.Series([lat, lon, row['location'].get('building_name', 'N/A')])\n",
    "        except Exception:\n",
    "            return pd.Series([None, None, None])\n",
    "\n",
    "    print(\"Convirtiendo coordenadas UTM a Lat/Lon (esto puede tardar un momento)...\")\n",
    "    df_ap_locations[['lat', 'lon', 'building_name']] = df_ap_locations.apply(convert_coordinates, axis=1)\n",
    "    df_ap_locations = df_ap_locations.dropna(subset=['lat', 'lon'])\n",
    "    df_ap_locations = df_ap_locations[['name', 'lat', 'lon', 'building_name']]\n",
    "    print(f\"Ubicaciones únicas de APs procesadas (total: {len(df_ap_locations)} APs).\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo {FILE_APS}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error procesando {FILE_APS}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Unir Métricas y Ubicaciones ---\n",
    "print(\"Uniendo métricas de clientes con ubicaciones de APs...\")\n",
    "df_master = pd.merge(df_metrics, df_ap_locations, on='name', how='inner')\n",
    "\n",
    "if df_master.empty:\n",
    "    print(\"Error: No se ha podido encontrar datos comunes entre clientes y APs.\")\n",
    "    exit()\n",
    "\n",
    "# Creamos el timestamp string en el dataframe maestro\n",
    "df_master['hour_str'] = df_master['hour'].astype(str).str.zfill(2)\n",
    "df_master['timestamp_str'] = df_master['date'].dt.strftime('%Y-%m-%d') + 'T' + df_master['hour_str'] + ':00:00'\n",
    "\n",
    "\n",
    "# --- 4. Preparar Datos para TimestampedGeoJson (¡MODIFICADO!) ---\n",
    "print(\"Creando 'scaffolding' de tiempo/AP para evitar 'stacking'...\")\n",
    "\n",
    "# Obtenemos todos los APs únicos y todos los tiempos únicos\n",
    "all_aps_data = df_ap_locations[['name', 'lat', 'lon', 'building_name']]\n",
    "all_times = df_master['timestamp_str'].unique()\n",
    "all_times.sort() # Nos aseguramos de que el tiempo esté ordenado\n",
    "\n",
    "# 1. Crear el \"andamio\" (scaffolding) con todas las combinaciones posibles\n",
    "df_scaffold_index = pd.MultiIndex.from_product([all_aps_data['name'].unique(), all_times], names=['name', 'timestamp_str'])\n",
    "df_scaffold = pd.DataFrame(index=df_scaffold_index).reset_index()\n",
    "\n",
    "# 2. Unir el andamio con los datos de AP (para tener lat/lon siempre)\n",
    "df_master_full = pd.merge(df_scaffold, all_aps_data, on='name', how='left')\n",
    "\n",
    "# 3. Unir con los datos de métricas (esto creará 'NaN' donde no haya datos)\n",
    "df_master_full = pd.merge(\n",
    "    df_master_full, \n",
    "    df_master, \n",
    "    on=['name', 'timestamp_str', 'lat', 'lon', 'building_name'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Formateando datos GeoJSON para los mapas dinámicos (Total features: {len(df_master_full)})...\")\n",
    "\n",
    "# --- Definir escalas de color y tamaño ---\n",
    "cmap_bueno_es_verde = branca.colormap.LinearColormap(['red', 'yellow', 'green'], vmin=0, vmax=100)\n",
    "max_clients_global = df_master['num_clients_metricos'].max()\n",
    "if pd.isna(max_clients_global) or max_clients_global == 0: max_clients_global = 1 \n",
    "cmap_mucho_es_rojo = branca.colormap.LinearColormap(['green', 'yellow', 'red'], vmin=0, vmax=max_clients_global)\n",
    "\n",
    "# Estilo INVISIBLE para APs sin datos\n",
    "style_invisible = {\n",
    "    'color': '#000000', 'fillColor': '#000000',\n",
    "    'opacity': 0.0, 'fillOpacity': 0.0, 'weight': 0, 'radius': 0\n",
    "}\n",
    "\n",
    "# --- Función para crear las \"features\" de GeoJSON (corregida) ---\n",
    "def create_feature(row, timestamp, iconstyle, popup):\n",
    "    \"\"\"Crea una única feature de GeoJSON para un punto en el tiempo.\"\"\"\n",
    "    return {\n",
    "        'type': 'Feature',\n",
    "        'geometry': {\n",
    "            'type': 'Point',\n",
    "            'coordinates': [row['lon'], row['lat']]\n",
    "        },\n",
    "        'properties': {\n",
    "            'time': timestamp,\n",
    "            'icon': 'circle',       \n",
    "            'iconstyle': iconstyle,   \n",
    "            'popup': popup\n",
    "        }\n",
    "    }\n",
    "\n",
    "features_health = []\n",
    "features_signal = []\n",
    "features_clients = []\n",
    "\n",
    "# Iteramos sobre el dataframe COMPLETO (df_master_full)\n",
    "for _, row in df_master_full.iterrows():\n",
    "    ts = row['timestamp_str']\n",
    "    \n",
    "    # Comprobamos si hay datos para esta hora/AP\n",
    "    is_active = not pd.isna(row['avg_health'])\n",
    "    \n",
    "    if is_active:\n",
    "        # --- Si está ACTIVO, creamos estilos VISIBLES ---\n",
    "        popup_html = (f\"<b>AP:</b> {row['name']}<br>\"\n",
    "                      f\"<b>Edificio:</b> {row['building_name']}<br>\"\n",
    "                      f\"<b>Hora:</b> {ts}<br>\"\n",
    "                      f\"<b>Health:</b> {row['avg_health']:.1f}<br>\"\n",
    "                      f\"<b>Señal:</b> {row['avg_signal_db']:.1f} dBm<br>\"\n",
    "                      f\"<b>Clientes:</b> {row['num_clients_metricos']}\")\n",
    "        \n",
    "        # 1. Estilo Health\n",
    "        health_color = cmap_bueno_es_verde(row['avg_health'])\n",
    "        style_health = {\n",
    "            'color': health_color, 'fillColor': health_color,\n",
    "            'opacity': 0.8, 'fillOpacity': 0.6, 'weight': 1, 'radius': 15\n",
    "        }\n",
    "        \n",
    "        # 2. Estilo Signal\n",
    "        signal_weight = (100 + row['avg_signal_db']) \n",
    "        signal_color = cmap_bueno_es_verde(signal_weight)\n",
    "        style_signal = {\n",
    "            'color': signal_color, 'fillColor': signal_color,\n",
    "            'opacity': 0.8, 'fillOpacity': 0.6, 'weight': 1, 'radius': 15\n",
    "        }\n",
    "\n",
    "        # 3. Estilo Clientes\n",
    "        client_radius = linear_scale(row['num_clients_metricos'], 0, max_clients_global, 5, 40)\n",
    "        client_color = cmap_mucho_es_rojo(row['num_clients_metricos'])\n",
    "        style_clients = {\n",
    "            'color': client_color, 'fillOpacity': 0.0, 'opacity': 0.7,\n",
    "            'weight': 3, 'radius': client_radius\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # --- Si está INACTIVO, creamos estilos INVISIBLES ---\n",
    "        popup_html = f\"<b>AP:</b> {row['name']}<br><b>Hora:</b> {ts}<br>Sin datos\"\n",
    "        \n",
    "        style_health = style_invisible\n",
    "        style_signal = style_invisible\n",
    "        style_clients = style_invisible\n",
    "    \n",
    "    # Añadimos la feature (visible o invisible)\n",
    "    features_health.append(create_feature(row, ts, style_health, popup_html))\n",
    "    features_signal.append(create_feature(row, ts, style_signal, popup_html))\n",
    "    features_clients.append(create_feature(row, ts, style_clients, popup_html))\n",
    "\n",
    "print(f\"Datos GeoJSON preparados.\")\n",
    "\n",
    "# --- 5. Función para crear y guardar los mapas (¡MODIFICADA!) ---\n",
    "def create_dynamic_bubble_map(features_list, ap_locations, output_filename, map_title):\n",
    "    print(f\"Creando mapa: {output_filename}...\")\n",
    "    \n",
    "    # --- ¡CAMBIO! Centramos en las coordenadas dadas con zoom 16 ---\n",
    "    m = folium.Map(location=map_center_coords, zoom_start=16)\n",
    "\n",
    "    # Capa 1: Marcadores de APs\n",
    "    fg_aps = folium.FeatureGroup(name='Mostrar Ubicación de APs')\n",
    "    offset_lat = 0.00003\n",
    "    offset_lon = 0.00004\n",
    "    for _, ap in ap_locations.iterrows():\n",
    "        bounds_rect = [\n",
    "            [ap['lat'] - offset_lat, ap['lon'] - offset_lon],\n",
    "            [ap['lat'] + offset_lat, ap['lon'] + offset_lon]\n",
    "        ]\n",
    "        folium.Rectangle(\n",
    "            bounds=bounds_rect,\n",
    "            color=\"#e63946\", fill=True, fill_color=\"#e63946\", fill_opacity=0.6,\n",
    "            popup=f\"<b>AP:</b> {ap['name']}<br><b>Edificio:</b> {ap['building_name']}\"\n",
    "        ).add_to(fg_aps)\n",
    "    fg_aps.add_to(m)\n",
    "\n",
    "    # Capa 2: Círculos Dinámicos\n",
    "    TimestampedGeoJson(\n",
    "        {'type': 'FeatureCollection', 'features': features_list},\n",
    "        period='PT1H', \n",
    "        duration='PT1H', # <-- ¡ARREGLO PARA \"STACKING\"! (Cada círculo dura 1h)\n",
    "        add_last_point=False, # <-- No dejar el último punto\n",
    "        auto_play=False,\n",
    "        loop=False,\n",
    "        max_speed=100, # <-- ¡VELOCIDAD AUMENTADA!\n",
    "        loop_button=True,\n",
    "        date_options='YYYY-MM-DD HH:mm',\n",
    "        time_slider_drag_update=True,\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Título\n",
    "    title_html = f'''\n",
    "                 <div style=\"position: fixed; top: 10px; left: 50px; z-index:1000;\n",
    "                             font-size: 24px; font-weight: bold; color: #1d3557;\n",
    "                             background-color: rgba(255, 255, 255, 0.7);\n",
    "                             padding: 5px 15px; border-radius: 5px;\">\n",
    "                   {map_title} (UAB)\n",
    "                 </div>\n",
    "                 '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "    folium.LayerControl().add_to(m)\n",
    "    m.save(output_filename)\n",
    "    print(f\"¡Mapa guardado! -> {output_filename}\")\n",
    "\n",
    "# --- 6. Generar los TRES mapas ---\n",
    "create_dynamic_bubble_map(\n",
    "    features_health,\n",
    "    df_ap_locations,\n",
    "    OUTPUT_MAP_HEALTH,\n",
    "    \"Mapa Dinámico: Health (Color: 0=Rojo, 100=Verde)\"\n",
    ")\n",
    "\n",
    "create_dynamic_bubble_map(\n",
    "    features_signal,\n",
    "    df_ap_locations,\n",
    "    OUTPUT_MAP_SIGNAL,\n",
    "    \"Mapa Dinámico: Señal (Color: Malo=Rojo, Bueno=Verde)\"\n",
    ")\n",
    "\n",
    "create_dynamic_bubble_map(\n",
    "    features_clients,\n",
    "    df_ap_locations,\n",
    "    OUTPUT_MAP_CLIENTS,\n",
    "    \"Mapa Dinámico: Nº Clientes (Tamaño: Dinámico | Borde: Verde-Rojo)\"\n",
    ")\n",
    "\n",
    "print(\"\\n¡Proceso completado! Revisa los TRES archivos .html generados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47a231",
   "metadata": {},
   "source": [
    "3. Lo implementamos mediante dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51ef57-b810-4ed5-a100-1736504ee922",
   "metadata": {},
   "source": [
    "__Què n’extraiem d’aquests heatmaps?__\n",
    "\n",
    "Tots tres ens aporten informació bàsica sobre les connexions com ara les hores punta i dies que freqüenten més gent. En podem extreure que, sens dubte, en dies lectius hi ha un flux major que no pas els caps de setmana gràcies a les icòniques aparicions en massa de circumferències i que són més ostentoses i visibles en horari acadèmic; les connexions són nombroses durant el matí i el migdia i es van reduint a mesura que es fa tard. A més a més, cada propietat ens aporta informació sobre possibles problemes a tenir en compte en la xarxa d’instal·lació d’APs. L’estudi en vers la potència de senyal ens permet localitzar punts-vall on no arriba bé el senyal; la investigació sobre el Health Score ens ajuda a identificar dificultats de connectivitat dels clients; i finalment el nombre de clients ens ensenya bàsicament la densitat d’usuaris per AP, el qual ens pot ajudar a redistribuir APs segons zones on hi hagi més o menys demanda per saturació.\n",
    "\n",
    "\n",
    "__Conclusions__\n",
    "\n",
    "Del primer heatmap concloem que la potència de senyal és bastant constant en tota la universitat, amb colors entre groc i taronja i potser algun valor atípic que torna un parell de circumferències roges o més verdes.\n",
    "\n",
    "Per Health Score, veiem que la majoria de les connexions són compatibles i positives, un vel que cobreix el campus majoritàriament verd. Tot i que momentàniament apareixen cercles grocs, taronges o rojos. \n",
    "\n",
    "En el cas de nombre de clients, podem veure la densitat d’usuaris per AP.\n",
    "\n",
    "Amb els mapes obtinguts hem intentat realitzar un mapa de calor en tres dimensions amb l’objectiu de relacionar la qualitat de connexió amb factors que podrien ser causants de la disminució d’aquesta com ara massa gent connectada al mateix AP. Així aprofitaríem la relació de dades com a mesura per descartar problemes alterns en vers altres variables. Desafortunadament, no ho hem pogut posar en pràctica, ja que els recursos necessaris per fer tal treball no eren compatibles amb les versions de les eines que posseíem.\n",
    "\n",
    "\n",
    "__Complicacions:__\n",
    "\n",
    "- Ordre de magnitud de dades massa grans per analitzar, emmagatzematge incompatible amb els ordinadors disponibles de l’equip.\n",
    "\n",
    "- Existència del retorn “nul” en una coordenada en filtrar les dades (potencial error en el moment de crear els heatmaps). Més tard, analitzant els heatmaps produïts, ens vam adonar que existia un sol AP en mig de l’oceà Atlàntic, el qual no té gens de sentit pensant que estem investigant repetidors en el domini de la UAB. D’això vam extreure la conclusió que coincidia amb les coordenades (0,0), possiblement a causa de la interpretació “nul” del codi.\n",
    "\n",
    "- Llibreries inservibles per a versions de Python més recents en l’intent de transferir informació per a fer un heatmap tridimensional.\n",
    "\n",
    "- En un moment més avançat del projecte, hem pogut córrer la simulació dels mapes de calor adquirits amb totes les dades subministrades. Vam detectar un problema en què s’stackejaven les àrees pintades, el qual dificultava la manipulació dels heatmaps amb els processadors disponibles.\n",
    "\n",
    "\n",
    "\n",
    "__Futures ampliacions:__\n",
    "La nostra idea és poder entrenar una intel·ligència artificial amb els heatmaps aconseguits perquè detecti automàticament zones problemàtiques mitjançant el criteri d’anàlisi seguit en el projecte. Així facilitaríem la recol·locació i l’addició d’APs en cas que es doni i també obtindríem prediccions sobre el posicionament ideal de dispositius i errors prevenibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eab25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
